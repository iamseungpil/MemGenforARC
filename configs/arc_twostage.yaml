# =============================================================================
# ARC Two-Stage Training Configuration
# =============================================================================
#
# This configuration implements the arc-lang-public two-stage architecture:
# - Stage 1: Generate instructions (with Weaver memory capture)
# - Stage 2: Generate grids using instructions + memory
#
# Key Features:
# - 3 instruction candidates per task (reduced for speed)
# - Single-example scoring (LOO disabled for ~4x speedup)
# - Memory persistence from instruction to grid generation
# - Grid similarity as the sole reward signal
#
# Speed Optimization Notes:
# - LOO disabled: 1 grid gen per candidate vs N (where N = num train examples)
# - Candidates reduced: 3 vs 5 (1.7x speedup)
# - Total estimated speedup: ~7x
#
# =============================================================================

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Base LLM - Qwen 3 14B (standard chat format, compatible with MemGen)
  model_name: Qwen/Qwen3-14B
  load_model_path: null

  # Load pre-trained weaver adapter from SFT checkpoint
  # This weaver has been trained on 7,185 high-quality instruction examples
  # Training: loss 0.85 -> 0.24, token accuracy 80% -> 92%
  load_weaver_path: ./outputs/arc_memgen_weaver_sft

  # Augmentation settings - Weaver memory injection
  max_prompt_aug_num: 1      # Single augmentation at instruction generation
  max_inference_aug_num: 3   # Multiple augmentations during grid generation

  # Weaver configuration (latent memory generator)
  weaver:
    model_name: Qwen/Qwen3-14B
    prompt_latents_len: 8     # Memory tokens for instruction capture
    inference_latents_len: 8  # Memory tokens for grid generation

    lora_config:
      r: 32                   # Larger rank for complex reasoning
      lora_alpha: 64
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      lora_dropout: 0.05
      bias: "none"
      task_type: "CAUSAL_LM"

  # Trigger configuration (disabled - always inject memory)
  trigger:
    model_name: Qwen/Qwen3-14B
    active: false

    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"

# =============================================================================
# ARC Two-Stage Specific Configuration
# =============================================================================
arc:
  # Number of instruction candidates per task
  # Reduced from 5 to 3 for faster training while maintaining GRPO diversity
  instruction_candidates: 3

  # Whether to use leave-one-out scoring for instruction validation
  # DISABLED for speed: uses single-example scoring instead (~4x faster)
  # When false: uses first example as test, rest as context
  leave_one_out_scoring: false

  # Maximum length for instruction generation
  max_instruction_length: 1024

  # Maximum length for grid generation
  max_grid_length: 4096

  # Temperature for instruction generation (higher = more diverse candidates)
  instruction_temperature: 0.8

  # Temperature for grid generation (lower = more precise)
  grid_temperature: 0.3

  # NOTE: Memory architecture follows arc-lang-public style:
  # - Memory flows ONLY between instruction generations (refinement loop)
  # - Grid generation uses pure text prompts (NO memory injection)

# =============================================================================
# Dataset Configuration
# =============================================================================
dataset:
  name: arc
  data_path: /home/ubuntu/arc-lang-public/data/arc-prize-2024
  mode: grpo

  # Multi-turn mode for two-stage training
  max_turns: 3
  num_seeds: 3

  sft:
    val_ratio: 0.1

  grpo:
    val_ratio: 0.1

# =============================================================================
# Training Configuration
# =============================================================================
run:
  seed: 42
  mode: train
  output_dir: ./outputs/arc_twostage

  # Training mode
  train_weaver: true
  train_weaver_method: grpo
  train_trigger: false
  train_trigger_method: grpo

  # Weaver GRPO training configuration
  weaver:
    grpo:
      # Disable vLLM (compatibility issue with current version)
      use_vllm: false

      # Training epochs and batch size
      num_train_epochs: 5
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      gradient_accumulation_steps: 4    # generation_batch_size = 2*4*2=16, divisible by num_generations=4

      # GRPO specific
      # num_generations must divide global batch size (num_processes * per_device_batch * grad_accum)
      # For 2 GPUs with batch=2, grad_accum=4: total=16, so num_generations=4 works
      # More generations = better reward variance for learning
      num_generations: 4

      beta: 0                             # KL penalty disabled (Dr. GRPO approach - projection layers have no reference state)

      # Sequence lengths
      max_prompt_length: 4096            # Long prompts for ARC examples
      max_completion_length: 1024        # Instructions can be verbose
      temperature: 0.9                    # Higher temp for more diverse samples (was 0.7)

      # Optimization
      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 2e-6                # Conservative LR for stability

      # Logging and saving
      logging_strategy: steps
      logging_steps: 5
      eval_strategy: steps
      eval_steps: 50
      save_strategy: steps
      save_steps: 100

      # Other settings
      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - wandb

  # Trigger training configuration (not used for ARC)
  trigger:
    grpo:
      num_train_epochs: 1
      per_device_train_batch_size: 1
      per_device_eval_batch_size: 1
      num_generations: 4
      num_iterations: 1
      gradient_accumulation_steps: 4
      beta: 0                              # KL penalty disabled (Dr. GRPO approach)
      loss_type: bnpo
      max_prompt_length: 4096
      max_completion_length: 1024
      temperature: 0.7
      optim: adamw_torch
      learning_rate: 5e-6
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 100
      save_strategy: steps
      save_steps: 100
      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - wandb

  # Interaction configuration for evaluation
  interaction:
    max_turns: 3                         # Match refinement_turns
    max_start_length: 4096
    max_prompt_length: 8192              # Allow long prompts with examples
    max_response_length: 1024
    max_obs_length: 512
    do_sample: true
    temperature: 0.9                      # Higher temp for diversity (was 0.7)
    batch_size: 2                        # Small batch for evaluation

# =============================================================================
# Alternative Configurations
# =============================================================================
# To use Qwen2.5-7B instead of Qwen3-14B, override:
#   --options model.model_name Qwen/Qwen2.5-7B-Instruct \
#             model.weaver.model_name Qwen/Qwen2.5-7B-Instruct \
#             model.trigger.model_name Qwen/Qwen2.5-7B-Instruct
#
# For higher quality (slower, original settings):
#   --options arc.instruction_candidates 5 \
#             arc.leave_one_out_scoring true \
#             run.weaver.grpo.num_generations 5 \
#             run.weaver.grpo.generation_batch_size 20 \
#             run.weaver.grpo.gradient_accumulation_steps 5
#
# For even faster training (minimal quality):
#   --options arc.instruction_candidates 2
