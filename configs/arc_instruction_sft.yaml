# =============================================================================
# ARC Instruction SFT Configuration
# =============================================================================
#
# SFT training for Weaver to learn instruction generation patterns.
# Uses high-quality instruction data from SOAR framework (training set).
#
# Key Features:
# - 7,185 instruction examples (score == 1.0, perfect)
# - 232 unique training tasks (GRPO compatible)
# - Messages format (TRL SFTTrainer compatible)
# - Training set data for proper train/eval split with GRPO
#
# =============================================================================

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # Base LLM - Qwen 3 14B
  model_name: Qwen/Qwen3-14B
  load_model_path: null

  # Augmentation settings - disabled for SFT (pure language modeling)
  max_prompt_aug_num: 0
  max_inference_aug_num: 0

  # Weaver configuration (latent memory generator)
  weaver:
    model_name: Qwen/Qwen3-14B
    prompt_latents_len: 8
    inference_latents_len: 8

    lora_config:
      r: 32
      lora_alpha: 64
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      lora_dropout: 0.05
      bias: "none"
      task_type: "CAUSAL_LM"

  # Trigger configuration (disabled)
  trigger:
    model_name: Qwen/Qwen3-14B
    active: false

    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"

# =============================================================================
# Dataset Configuration
# =============================================================================
dataset:
  name: arc
  data_path: /home/ubuntu/arc-lang-public/data/arc-prize-2024
  mode: sft

  # Use instruction SFT data from evaluation set
  use_instruction_sft: true

  # Single-turn for SFT
  max_turns: 1
  num_seeds: 1

  sft:
    val_ratio: 0.1

  grpo:
    val_ratio: 0.1

# =============================================================================
# Training Configuration
# =============================================================================
run:
  seed: 42
  mode: train
  output_dir: ./outputs/arc_instruction_sft

  # Training mode - SFT for Weaver
  train_weaver: true
  train_weaver_method: sft
  train_trigger: false
  train_trigger_method: grpo

  # Weaver SFT training configuration
  weaver:
    sft:
      # Reduced epochs for larger dataset (7185 examples from soar2cot)
      num_train_epochs: 3
      per_device_train_batch_size: 4
      per_device_eval_batch_size: 4
      gradient_accumulation_steps: 2

      # Sequence lengths
      max_seq_length: 4096

      # Optimization - conservative for small dataset
      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 1e-5

      # Logging and saving
      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 50
      save_strategy: steps
      save_steps: 100

      # Other settings
      remove_unused_columns: true  # Remove task_id, score columns
      load_best_model_at_end: true
      bf16: true
      packing: false

      # Disable wandb for now (can enable later)
      report_to:
        - wandb

    # GRPO config (for later use after SFT)
    grpo:
      num_train_epochs: 5
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      gradient_accumulation_steps: 3
      num_generations: 3
      generation_batch_size: 12
      num_iterations: 1
      beta: 0.01
      loss_type: grpo
      max_prompt_length: 4096
      max_completion_length: 1024
      temperature: 0.7
      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 2e-6
      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 50
      save_strategy: steps
      save_steps: 100
      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - wandb

  # Trigger training configuration (not used)
  trigger:
    grpo:
      num_train_epochs: 1
      per_device_train_batch_size: 1
      per_device_eval_batch_size: 1
      num_generations: 4
      num_iterations: 1
      gradient_accumulation_steps: 4
      beta: 0.01
      loss_type: bnpo
      max_prompt_length: 4096
      max_completion_length: 1024
      temperature: 0.7
      optim: adamw_torch
      learning_rate: 5e-6
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 100
      save_strategy: steps
      save_steps: 100
      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - wandb

  # Interaction configuration for evaluation
  interaction:
    max_turns: 1
    max_start_length: 4096
    max_prompt_length: 8192
    max_response_length: 1024
    max_obs_length: 512
    do_sample: true
    temperature: 0.7
    batch_size: 2
