# ARC Training Configuration for MemGen
# SFT Warmup for Weaver before GRPO

model:
  # Base LLM - Qwen3-14B for better reasoning
  model_name: Qwen/Qwen3-14B
  load_model_path: null

  # Augmentation settings
  max_prompt_aug_num: 1      # Single augmentation at prompt end
  max_inference_aug_num: 5   # Multiple augmentations during grid generation

  # Weaver configs (latent memory generator)
  weaver:
    model_name: Qwen/Qwen3-14B
    prompt_latents_len: 8
    inference_latents_len: 8

    lora_config:
      r: 32                    # Larger rank for 14B model
      lora_alpha: 64
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      lora_dropout: 0.05
      bias: "none"
      task_type: "CAUSAL_LM"

  # Trigger configs (disabled - always inject)
  trigger:
    model_name: Qwen/Qwen3-14B
    active: false

    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"

# Dataset configs
dataset:
  name: arc
  data_path: /home/ubuntu/arc-lang-public/data/arc-prize-2024
  mode: sft  # SFT warmup before GRPO
  sft:
    val_ratio: 0.1
  grpo:
    val_ratio: 0.1

# Training configs
run:
  seed: 42

  # LTPO (Latent Thought Policy Optimization) configs for test-time optimization
  ltpo:
    enabled: true
    lr: 0.03
    sigma: 0.1
    sigma_decay: 0.99
    max_steps: 10
    reward_threshold: -1.0
    top_k: 10
    use_auto_grad: true
    verbose: false

  # Training mode
  mode: train
  train_weaver: true
  train_weaver_method: sft     # SFT warmup before GRPO
  train_trigger: false
  train_trigger_method: grpo
  output_dir: ./outputs/arc_sft_warmup

  # Weaver training configs
  weaver:

    # SFT configs (optional pretraining)
    sft:
      num_train_epochs: 2
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      gradient_accumulation_steps: 4

      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 1e-5

      logging_strategy: steps
      logging_steps: 10
      eval_strategy: epoch
      save_strategy: "no"  # Disable auto-save to avoid shared tensor issues

      max_length: 2048
      remove_unused_columns: false
      load_best_model_at_end: false  # Cannot use with save_strategy: "no"
      bf16: true
      report_to:
        - tensorboard

    # GRPO configs (main training with grid_similarity reward)
    grpo:
      num_train_epochs: 3
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      num_generations: 4       # Generate 4 samples per prompt
      num_iterations: 1
      gradient_accumulation_steps: 4
      beta: 0.01               # KL penalty
      loss_type: grpo

      max_prompt_length: 2048
      max_completion_length: 512  # Grid output is relatively short
      temperature: 0.7

      optim: adamw_torch
      lr_scheduler_type: cosine
      warmup_ratio: 0.1
      learning_rate: 5e-6

      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 100
      save_strategy: steps
      save_steps: 100

      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - tensorboard

  # Trigger training configs (not used for ARC)
  trigger:
    grpo:
      num_train_epochs: 1
      per_device_train_batch_size: 2
      per_device_eval_batch_size: 2
      num_generations: 4
      num_iterations: 1
      gradient_accumulation_steps: 4
      beta: 0.01
      loss_type: bnpo

      max_prompt_length: 2048
      max_completion_length: 512
      temperature: 0.7

      optim: adamw_torch
      learning_rate: 5e-6
      lr_scheduler_type: cosine
      warmup_ratio: 0.1

      logging_strategy: steps
      logging_steps: 10
      eval_strategy: steps
      eval_steps: 100
      save_strategy: steps
      save_steps: 100

      remove_unused_columns: false
      load_best_model_at_end: true
      bf16: true
      report_to:
        - tensorboard

  # Evaluation interaction config
  interaction:
    max_turns: 1
    max_start_length: 2048
    max_prompt_length: 4096
    max_response_length: 512
    max_obs_length: 256
    do_sample: false
    temperature: 0.0
    batch_size: 4
