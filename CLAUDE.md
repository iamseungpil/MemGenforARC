# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

MemGen (Memory Generator) is a framework for self-evolving AI agents that generates latent memory tokens within the model's reasoning stream. It consists of two core modules:
- **Memory Weaver**: Synthesizes past experiences into compact latent sequences for reasoning augmentation
- **Memory Trigger**: Decides when to recall and insert memory during generation

## Common Commands

### Environment Setup
```bash
conda create -n memgen python=3.10
conda activate memgen
pip install -r requirements.txt
```

### Training

**Train Weaver model (SFT or GRPO):**
```bash
bash scripts/weaver_train.sh
```

**Train Trigger model (GRPO only):**
```bash
bash scripts/trigger_train.sh
```

### Evaluation
```bash
# Update LOAD_MODEL_PATH in eval.sh first
bash scripts/eval.sh
```

### Running with Custom Config
```bash
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/<dataset>.yaml \
    --options <key> <value> ...
```

## Architecture

### Core Components

**MemGenModel** (`memgen/model/modeling_memgen.py`):
- Main model class inheriting from `PreTrainedModel`
- Contains three sub-models: `reasoner` (base LLM), `weaver`, and `trigger`
- Uses LoRA adapters for weaver and trigger to avoid full fine-tuning
- Projection layers (`reasoner_to_weaver`, `weaver_to_reasoner`) map embeddings between components

**MemGenWeaver** (`memgen/model/weaver.py`):
- Generates latent memory tokens via learnable query latents
- Two modes: `augment_prompt()` for prompt-end augmentation, `augment_inference()` for mid-generation augmentation
- Uses `prompt_query_latents` and `inference_query_latents` as trainable parameters

**MemGenTrigger** (`memgen/model/trigger.py`):
- Binary classifier deciding whether to insert memory at each position
- Output layer maps hidden states to 2-class logits (insert/skip)
- When `active=False`, always returns logits favoring insertion

**MemGenRunner** (`memgen/runner.py`):
- Orchestrates training and evaluation
- Two-stage training: weaver first, then trigger
- Supports SFT and GRPO training methods for weaver, GRPO only for trigger

### Data Pipeline

**BaseBuilder** (`data/base_builder.py`):
- Abstract class for dataset construction
- Returns `DatasetDict` with train/valid/test splits
- Provides environment class via `get_env_cls()`

**BaseEnv** (`data/base_env.py`):
- Two environment types: `StaticEnv` (single-turn) and `DynamicEnv` (multi-turn)
- `compute_reward()` method for RL training
- Dynamic envs implement `step()`, `set_env()`, and `feedback()` for interaction loops

**Supported Datasets** (in `data/`):
- `gsm8k`: Math word problems (Static)
- `gpqa`: Graduate-level QA (Static)
- `kodcode`: Code generation (Static)
- `triviaqa`: Retrieval-augmented QA (Dynamic, multi-turn)

### Interaction System

**InteractionManager** (`interactions/base_interaction.py`):
- Manages model generation during training/evaluation
- `SingleTurnInteractionManager`: For static environments
- `MultiTurnInteractionManager`: For dynamic environments with tool use

### Configuration

YAML configs in `configs/latent_memory/` define:
- `model`: Base LLM, weaver/trigger settings, LoRA configs, augmentation parameters
- `dataset`: Dataset name, mode (sft/grpo), validation ratio
- `run`: Training mode, trainer configs (SFT/GRPO hyperparameters), interaction settings

Key augmentation parameters:
- `max_prompt_aug_num`: Number of prompt-end augmentations (1 for reasoning tasks, 6+ for retrieval)
- `max_inference_aug_num`: Number of mid-generation augmentations (5 for reasoning, 0 for retrieval)
- `prompt_latents_len`, `inference_latents_len`: Length of latent sequences

## Key Implementation Details

- Models use `bfloat16` precision and Flash Attention 2
- Training uses Accelerate with DeepSpeed ZeRO-2 (`configs/zero2.yaml`)
- Weaver training fixes trigger params and vice versa via `fix_component()`/`open_component()`
- Multi-turn forward processes conversation turns sequentially, with latents not visible across turns
- Generation loop interleaves trigger decisions with weaver augmentation at delimiter positions

---

## ARC Experiment (2025-01 Update)

### Overview

ARC (Abstract Reasoning Corpus) ì‹¤í—˜ì„ ìœ„í•œ ì½”ë“œ ìƒì„± ê¸°ë°˜ ì ‘ê·¼ë²•. BARC-styleë¡œ ëª¨ë¸ì´ Python ì½”ë“œë¥¼ ìƒì„±í•˜ê³ , training examplesì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ì •í™•ë„ë¡œ rewardë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

### Three Main Execution Modes

#### 1. Pretrain (Weaver SFT/GRPO)
Weaver ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ latent memory ìƒì„± ëŠ¥ë ¥ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

```bash
# SFT Warmup (ê¶Œì¥: GRPO ì „ ì‚¬ì „í•™ìŠµ)
bash scripts/arc_train.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
GPU_IDS=0,1 python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    --num_processes=2 \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode train \
    run.train_weaver true \
    run.train_weaver_method sft  # or grpo
```

**í•™ìŠµ ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.train()` â†’ `_train_weaver()`
2. Weaverì˜ LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ (trigger ê³ ì •)
3. SFT: supervised learningìœ¼ë¡œ latent ìƒì„± í•™ìŠµ
4. GRPO: code execution accuracyë¥¼ rewardë¡œ ê°•í™”í•™ìŠµ

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/train/arc/<model_name>/`

#### 2. Eval (Standard Evaluation)
í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” base ëª¨ë¸ì˜ ARC ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

```bash
bash scripts/eval.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode evaluate \
    model.load_model_path <checkpoint_path>
```

**í‰ê°€ ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.evaluate()` â†’ `_static_evaluate()`
2. Weaverê°€ latent tokens ìƒì„±
3. Reasonerê°€ latent + promptë¡œ Python ì½”ë“œ ìƒì„±
4. ì½”ë“œ ì‹¤í–‰í•˜ì—¬ training examples ì •í™•ë„ ê³„ì‚°

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/evaluate/arc/<model_name>/evaluate/answer.json`

#### 3. Test-Time Train with LTPO
LTPO (Latent Thought Policy Optimization)ë¥¼ ì‚¬ìš©í•˜ì—¬ inference ì‹œ latentë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.

```bash
bash scripts/eval_ltpo.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode evaluate_ltpo \
    run.ltpo.enabled true \
    run.ltpo.lr 0.03 \
    run.ltpo.max_steps 10
```

**LTPO ìµœì í™” ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.evaluate_with_ltpo()` â†’ `_static_evaluate_with_ltpo()`
2. Weaverê°€ ì´ˆê¸° latent hidden states ìƒì„±
3. `MemGenLTPOOptimizer.optimize()`:
   - ì´ˆê¸° latentì— noise ì¶”ê°€ (exploration)
   - confidence reward ê³„ì‚° (top-k token probability)
   - gradient ascentë¡œ latent ì—…ë°ì´íŠ¸
   - max_stepsë§Œí¼ ë°˜ë³µ
4. ìµœì í™”ëœ latentë¡œ ì½”ë“œ ìƒì„±

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/evaluate_ltpo/arc/<model_name>/evaluate/answer_ltpo.json`

### LTPO Module (`ltpo/`)

| íŒŒì¼ | ì—­í•  |
|------|------|
| `ltpo.py` | ì›ë³¸ LTPO êµ¬í˜„ (standalone) |
| `memgen_ltpo.py` | MemGen í†µí•© LTPO ìµœì í™”ê¸° |
| `reward.py` | Reward model ì¸í„°í˜ì´ìŠ¤ |

**í•µì‹¬ íŒŒë¼ë¯¸í„° (`configs/latent_memory/arc.yaml`):**
```yaml
run:
  ltpo:
    enabled: true        # LTPO í™œì„±í™”
    lr: 0.03             # ìµœì í™” learning rate
    sigma: 0.1           # exploration noise std
    sigma_decay: 0.99    # noise decay per step
    max_steps: 10        # ìµœëŒ€ ìµœì í™” ìŠ¤í…
    reward_threshold: -1 # early stopping threshold (-1=disabled)
    top_k: 10            # confidence ê³„ì‚°ìš© top-k tokens
    use_auto_grad: true  # PyTorch autograd ì‚¬ìš© (vs REINFORCE)
```

### ARC Environment (`data/arc/env.py`)

| í´ë˜ìŠ¤ | íƒ€ì… | ìš©ë„ |
|--------|------|------|
| `ARCEnv` | Static | Single-turn ì½”ë“œ ìƒì„± + í‰ê°€ |
| `ARCDynamicEnv` | Dynamic | Multi-turn ì½”ë“œ refinement |

**Reward ê³„ì‚°:**
- Binary reward: ALL training examples í†µê³¼ â†’ 1.0, otherwise â†’ 0.0
- `validate_code_on_examples()`: ì½”ë“œ íŒŒì‹± â†’ ì‹¤í–‰ â†’ ì •í™•ë„ ê³„ì‚°

### ì„¤ì • íŒŒì¼

**`configs/latent_memory/arc.yaml`:**
```yaml
model:
  model_name: Qwen/Qwen3-14B
  max_prompt_aug_num: 1      # prompt ë latent ê°œìˆ˜
  max_inference_aug_num: 5   # ìƒì„± ì¤‘ latent ì‚½ì… íšŸìˆ˜
  weaver:
    prompt_latents_len: 8    # prompt latent ê¸¸ì´
    inference_latents_len: 8 # inference latent ê¸¸ì´

dataset:
  name: arc
  data_path: /home/ubuntu/arc-lang-public/data/arc-prize-2024

run:
  mode: train/evaluate/evaluate_ltpo
```

### ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Training Flow                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. arc_train.sh â†’ main.py (mode=train)                         â”‚
â”‚  2. MemGenRunner.train() â†’ _train_weaver()                      â”‚
â”‚  3. WeaverGRPOTrainer: prompt â†’ weaver latents â†’ code generationâ”‚
â”‚  4. ARCEnv.compute_reward(): execute code â†’ accuracy â†’ reward   â”‚
â”‚  5. GRPO loss: optimize weaver LoRA parameters                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Evaluation Flow                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. eval.sh â†’ main.py (mode=evaluate)                           â”‚
â”‚  2. MemGenRunner.evaluate() â†’ _static_evaluate()                â”‚
â”‚  3. Weaver generates latents â†’ Reasoner generates code          â”‚
â”‚  4. StaticEvalRecorder: compute_reward() â†’ log results          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LTPO Test-Time Flow                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. eval_ltpo.sh â†’ main.py (mode=evaluate_ltpo)                 â”‚
â”‚  2. MemGenRunner.evaluate_with_ltpo()                           â”‚
â”‚  3. Create MemGenLTPOOptimizer                                  â”‚
â”‚  4. For each sample:                                            â”‚
â”‚     a. Weaver â†’ initial latents                                 â”‚
â”‚     b. LTPO loop: noise â†’ confidence reward â†’ gradient update   â”‚
â”‚     c. Optimized latents â†’ code generation                      â”‚
â”‚  5. Log results to answer_ltpo.json                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” ì½”ë“œ íŒŒì¼

| íŒŒì¼ | ì—­í•  |
|------|------|
| `main.py` | ì§„ì…ì , modeì— ë”°ë¼ train/evaluate/evaluate_ltpo ë¶„ê¸° |
| `memgen/runner.py` | Training/Evaluation orchestration |
| `memgen/model/modeling_memgen.py` | MemGenModel (reasoner + weaver + trigger) |
| `memgen/model/weaver.py` | Latent memory ìƒì„± (augment_prompt/augment_inference) |
| `ltpo/memgen_ltpo.py` | Test-time latent optimization |
| `data/arc/env.py` | ARC environment + reward computation |
| `data/arc/builder.py` | ARC dataset builder |
| `arc/utils.py` | ì½”ë“œ íŒŒì‹±/ì‹¤í–‰/ê²€ì¦ ìœ í‹¸ë¦¬í‹° |

### ë””ë²„ê¹… íŒ

1. **LTPO ìµœì í™” í™•ì¸:** `run.ltpo.verbose: true`ë¡œ ì„¤ì •í•˜ë©´ ê° stepì˜ reward ì¶œë ¥
2. **ì½”ë“œ ì‹¤í–‰ ì—ëŸ¬:** `arc/utils.py`ì˜ `validate_code_on_examples()` ë¡œê·¸ í™•ì¸
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±:** `max_prompt_aug_num`, `max_inference_aug_num` ì¤„ì´ê¸°
4. **Rewardê°€ 0:** training examples JSON íŒŒì‹± í™•ì¸, ì½”ë“œ ë¸”ë¡ í˜•ì‹ í™•ì¸

---

## âš ï¸ ì¤‘ìš” ê°œë… ì •ë¦¬ (2025-01-04)

### Test-Time Optimization vs Test-Time Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    âš ï¸ LTPOëŠ” Test-Time OPTIMIZATIONì´ë‹¤!                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  Test-Time Training (TTT)         â”‚  Test-Time Optimization (LTPO)   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â€¢ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ O          â”‚  â€¢ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ X         â”‚
â”‚  â€¢ ì˜êµ¬ì  ë³€ê²½                    â”‚  â€¢ inference ì‹œì—ë§Œ ì„ì‹œ ìµœì í™”    â”‚
â”‚  â€¢ ë³„ë„ êµ¬í˜„ í•„ìš”                 â”‚  â€¢ eval_ltpo.shë¡œ ì‹¤í–‰            â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**LTPOê°€ ìµœì í™”í•˜ëŠ” ê²ƒ:**
- Weaverê°€ ìƒì„±í•œ `latent_hidden_states` (embedding ë²¡í„°)
- ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ **ì¤‘ê°„ í‘œí˜„(latent embeddings)**ë§Œ ìµœì í™”
- ê° ìƒ˜í”Œë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”, ìƒ˜í”Œ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ

**LTPO Reward:**
- confidence-based reward (top-k token probability)
- ARC binary reward (code execution accuracy)ì™€ **ë³„ê°œ**

### Binary Reward êµ¬í˜„ (ARC ì „ìš©)

```python
# data/arc/env.py - ARCEnv.compute_reward()
if accuracy == 1.0:   # ëª¨ë“  training examples í†µê³¼
    reward = 1.0
else:                 # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨
    reward = 0.0
```

**ì´ìœ :** ARCì—ì„œ ë¶€ë¶„ ì •ë‹µ(2/3 ë§ìŒ)ì€ ì™„ì „ ì˜¤ë‹µê³¼ ë™ì¼ - ê·œì¹™ì´ ì™„ì „íˆ ë§ê±°ë‚˜ ì™„ì „íˆ í‹€ë¦¬ê±°ë‚˜

### ì„¸ ê°€ì§€ íŒŒì´í”„ë¼ì¸ í•µì‹¬ ì •ë¦¬

| íŒŒì´í”„ë¼ì¸ | ìŠ¤í¬ë¦½íŠ¸ | ëª¨ë¸ ì—…ë°ì´íŠ¸ | ì‚¬ìš© ëª©ì  |
|-----------|---------|-------------|----------|
| **Training** | `weaver_train.sh` | âœ… Yes (LoRA) | Weaver/Trigger í•™ìŠµ |
| **Evaluation** | `eval.sh` | âŒ No | ì„±ëŠ¥ ì¸¡ì • |
| **LTPO Eval** | `eval_ltpo.sh` | âŒ No | Latent ìµœì í™” í›„ í‰ê°€ |

---

## ğŸ”§ ìµœê·¼ ìˆ˜ì • ì‚¬í•­ (2025-01-04)

### Critical Fixes Applied

| # | íŒŒì¼ | ì´ìŠˆ | ìˆ˜ì • |
|---|------|------|------|
| 1 | `memgen/runner.py:110-123` | `_filter_dataset()` evaluate ëª¨ë“œ crash | `interaction_config` fallback ì¶”ê°€ |
| 2 | `data/base_env.py:29` | `preprocess_action(self,...)` | `self` â†’ `cls` (classmethod) |
| 3 | `memgen/trainer/trigger_grpo_trainer.py` | Missing imports | `SamplingParams`, `gather`, `is_conversational` ì¶”ê°€ |
| 4 | `memgen/trainer/trigger_grpo_trainer.py:126-181` | Missing method | `_calculate_rewards()` ë©”ì„œë“œ ì¶”ê°€ |

### ì‚­ì œëœ ì½”ë“œ (ì˜ë„ì )

| íŒŒì¼/í´ë˜ìŠ¤ | ì´ìœ  |
|------------|------|
| `ARCCodeEnv` | `ARCEnv`ì™€ ì¤‘ë³µ (ë™ì¼ ê¸°ëŠ¥) |
| `configs/arc_twostage.yaml` | 2-stage training ë¯¸ì‚¬ìš© |
| `configs/arc_instruction_sft.yaml` | instruction â†’ code ë°©ì‹ ì „í™˜ |
| `interactions/arc_multiturn_interaction.py` | í˜„ì¬ single-turnë§Œ ì‚¬ìš© |

### ìœ ì§€í•´ì•¼ í•  ì½”ë“œ (ì‚­ì œ ê¸ˆì§€!)

| íŒŒì¼ | ì´ìœ  |
|------|------|
| `data/triviaqa/` | ë‹¤ë¥¸ ì‹¤í—˜ìš© dynamic env |
| `interactions/multiturn_interaction.py` | TriviaQA ë“± multi-turn ì§€ì› |
| `ARCDynamicEnv` | í–¥í›„ multi-turn ARC í™•ì¥ìš© |
| `ltpo/` ì „ì²´ | Test-time optimization í•µì‹¬ |

---

## ğŸ“ ARC Single-Turn Code Generation ì ‘ê·¼ë²•

### ì™œ Code Generationì¸ê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BARC-Style Approach                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ê¸°ì¡´ ë°©ì‹ (Instruction)          â”‚  í˜„ì¬ ë°©ì‹ (Code Generation)     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  "ìƒë‹¨ 2ì¤„ì„ í•˜ë‹¨ìœ¼ë¡œ ë³µì‚¬"         â”‚  def main(input_grid):          â”‚
â”‚  â†’ ëª¨í˜¸í•œ ìì—°ì–´ ì§€ì‹œ              â”‚      return input_grid[:2]       â”‚
â”‚  â†’ ì‹¤í–‰ ë¶ˆê°€                      â”‚  â†’ ëª…í™•í•œ ì½”ë“œ                    â”‚
â”‚  â†’ í‰ê°€ ì–´ë ¤ì›€                    â”‚  â†’ ì‹¤í–‰ ê°€ëŠ¥                      â”‚
â”‚                                   â”‚  â†’ ì •í™•ë„ë¡œ í‰ê°€                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° íë¦„

```
ARC Task JSON
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCBuilder     â”‚ â†’ training examplesë¥¼ promptë¡œ ë³€í™˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt Example:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  Example 1:                                                      â”‚
â”‚  Input (3x3):                                                    â”‚
â”‚  0 0 1                                                           â”‚
â”‚  0 1 0                                                           â”‚
â”‚  1 0 0                                                           â”‚
â”‚                                                                  â”‚
â”‚  Output (3x3):                                                   â”‚
â”‚  1 0 0                                                           â”‚
â”‚  0 1 0                                                           â”‚
â”‚  0 0 1                                                           â”‚
â”‚                                                                  â”‚
â”‚  Write a Python function `main(input_grid)` that implements...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MemGen Model   â”‚ â†’ Weaver latents + Reasoner generation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generated Code:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  ```python                                                       â”‚
â”‚  def main(input_grid):                                           â”‚
â”‚      import numpy as np                                          â”‚
â”‚      grid = np.array(input_grid)                                 â”‚
â”‚      return np.flip(grid, axis=1).tolist()                       â”‚
â”‚  ```                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCEnv Reward  â”‚ â†’ ì½”ë“œ ì‹¤í–‰ â†’ training examples ì •í™•ë„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Binary Reward: 1.0 (all pass) or 0.0 (any fail)
```

### ì½”ë“œ ì‹¤í–‰ íë¦„ (`arc/utils.py`)

```python
# 1. ì½”ë“œ íŒŒì‹±
code = parse_code_from_text(completion)  # ```python ... ``` ì¶”ì¶œ

# 2. ê° training exampleì—ì„œ ì‹¤í–‰
for example in train_examples:
    result = execute_code_on_input(code, example["input"])
    if result == example["output"]:
        passed += 1

# 3. ì •í™•ë„ ê³„ì‚°
accuracy = passed / total

# 4. Binary reward
reward = 1.0 if accuracy == 1.0 else 0.0
```

---

## ğŸ›¡ï¸ ìˆ˜ì • ì‹œ ì£¼ì˜ì‚¬í•­

### ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ë§ ê²ƒ
1. `ltpo/memgen_ltpo.py` - LTPO í•µì‹¬ ë¡œì§
2. `data/arc/env.py`ì˜ binary reward ë¡œì§
3. `memgen/runner.py`ì˜ `evaluate_with_ltpo()` ë©”ì„œë“œ
4. `main.py`ì˜ mode ë¶„ê¸° ë¡œì§

### ìˆ˜ì • ì „ í™•ì¸í•  ê²ƒ
1. **Import chain**: ìˆœí™˜ ì°¸ì¡° í™•ì¸ (`arc/__init__.py` ì£¼ì˜)
2. **Type signatures**: `@classmethod`ëŠ” `cls` ì‚¬ìš©
3. **Trainer methods**: `_calculate_rewards()` ì¡´ì¬ í™•ì¸
4. **Config keys**: YAML í‚¤ì™€ ì½”ë“œ íŒŒë¼ë¯¸í„°ëª… ì¼ì¹˜ í™•ì¸

### í…ŒìŠ¤íŠ¸ ë°©ë²•
```bash
# ëª¨ë“  import ê²€ì¦
python -c "from memgen.runner import MemGenRunner; from data.arc.env import ARCEnv, ARCDynamicEnv; from ltpo import MemGenLTPOOptimizer; print('OK')"

# LTPO ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
python -c "from memgen.runner import MemGenRunner; assert hasattr(MemGenRunner, 'evaluate_with_ltpo'); print('OK')"
```
