# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

MemGen (Memory Generator) is a framework for self-evolving AI agents that generates latent memory tokens within the model's reasoning stream. It consists of two core modules:
- **Memory Weaver**: Synthesizes past experiences into compact latent sequences for reasoning augmentation
- **Memory Trigger**: Decides when to recall and insert memory during generation

## ğŸš¨ ê°œë°œ ì›ì¹™ (2025-01-08)

### Master Branch ë³´í˜¸ ì›ì¹™
1. **í•­ìƒ master branchì™€ ë¹„êµí•˜ë©° ì‘ì—…**
   - `git diff origin/master --stat`ë¡œ ë³€ê²½ ë²”ìœ„ í™•ì¸
   - ë¶ˆí•„ìš”í•œ ë³€ê²½ ìµœì†Œí™”

2. **master ì½”ë“œ ë³€ê²½ ìµœì†Œí™”**
   - ë°˜ë“œì‹œ í•„ìš”í•˜ê±°ë‚˜ ëª…ì‹œì  ìš”ì²­ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ì •
   - ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •ë³´ë‹¤ ìƒˆ íŒŒì¼/í•¨ìˆ˜ ì¶”ê°€ ì„ í˜¸

3. **ë³€ê²½ ì „ í™•ì¸ ì‚¬í•­**
   - í•´ë‹¹ ë³€ê²½ì´ ì •ë§ í•„ìš”í•œê°€?
   - ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ê°€?
   - ë” ì‘ì€ ë²”ìœ„ë¡œ í•´ê²° ê°€ëŠ¥í•œê°€?

## ğŸ”§ ì›ë³¸ MemGen ë³µì› ì‘ì—… (2025-01-08)

ltpo ë¸Œëœì¹˜ì—ì„œ ì›ë³¸ master ëŒ€ë¹„ ë³€ê²½ë˜ì—ˆë˜ ë¶€ë¶„ì„ ë³µì›í•œ ë‚´ì—­:

### 1. `_grpo_forward` ë©”ì„œë“œ ì‚­ì œ (`modeling_memgen.py`)
- **ë¬¸ì œ**: ltpo ë¸Œëœì¹˜ì—ì„œ `_grpo_forward` ë©”ì„œë“œê°€ ìƒˆë¡œ ì¶”ê°€ë¨
- **ì›ì¸**: prompt augmentationë§Œ ìˆ˜í–‰í•˜ê³ , inference augmentation(`_select_augment_points_after_delimiter`)ì„ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
- **ê²°ê³¼**: GRPO í•™ìŠµ ì‹œ latent memoryê°€ prompt ëì—ë§Œ ì‚½ì…ë˜ê³ , ìƒì„± ì¤‘ê°„ì— ì‚½ì…ë˜ì§€ ì•ŠìŒ
- **ìˆ˜ì •**: `_grpo_forward` ë©”ì„œë“œ ì‚­ì œ, ì›ë³¸ì²˜ëŸ¼ `_forward` ì‚¬ìš©

### 2. `is_grpo` í”Œë˜ê·¸ ì‚­ì œ (`modeling_memgen.py`, `weaver_grpo_trainer.py`)
- **ë¬¸ì œ**: `forward()`ì—ì„œ `is_grpo=True`ë©´ `_grpo_forward` í˜¸ì¶œí•˜ëŠ” ë¶„ê¸° ì¶”ê°€ë¨
- **ì›ì¸**: trainerì—ì„œ `"is_grpo": True`ë¥¼ ì „ë‹¬í•˜ì—¬ ìœ„ì˜ ë¶ˆì™„ì „í•œ `_grpo_forward` ì‚¬ìš©
- **ìˆ˜ì •**: `is_grpo` ì²´í¬ ë¡œì§ ì‚­ì œ, trainerì—ì„œ `"is_grpo": True` ì „ë‹¬ ì‚­ì œ

### 3. `compute_loss` ë©”ì„œë“œ ì£¼ì„ì²˜ë¦¬ (`weaver_grpo_trainer.py`)
- **ë¬¸ì œ**: `compute_loss` ë©”ì„œë“œê°€ ìƒˆë¡œ ì˜¤ë²„ë¼ì´ë“œë¨
- **ì›ì¸**: loss ê³„ì‚° ê³µì‹ì´ GRPOê°€ ì•„ë‹Œ BNPO ë°©ì‹ ì‚¬ìš©
  - GRPO: `((per_token_loss * mask).sum(-1) / mask.sum(-1)).mean()` (ìƒ˜í”Œë³„ ì •ê·œí™” í›„ í‰ê· )
  - BNPO (ì˜ëª»ë¨): `(per_token_loss * mask).sum() / mask.sum()` (ì „ì²´ ì •ê·œí™”)
- **ìˆ˜ì •**: `compute_loss` ë©”ì„œë“œ ì „ì²´ ì£¼ì„ì²˜ë¦¬, ì›ë³¸ `_compute_loss` ì‚¬ìš©

### 4. projection layer dtype ì œê±° (`modeling_memgen.py`)
- **ë¬¸ì œ**: `reasoner_to_weaver`, `weaver_to_reasoner` Linear ë ˆì´ì–´ì— `dtype=torch.bfloat16` ì¶”ê°€ë¨
- **ì›ì¸**: í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°ê°€ bfloat16ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ì •ë°€ë„ ì €í•˜
- **ì›ë³¸**: dtype ë¯¸ì§€ì • (ê¸°ë³¸ float32)
- **ìˆ˜ì •**: `dtype=torch.bfloat16` ì œê±°

### 5. query_latents dtype ì œê±° (`weaver.py`)
- **ë¬¸ì œ**: `prompt_query_latents`, `inference_query_latents`ì— `dtype=torch.bfloat16` ì¶”ê°€ë¨
- **ì›ì¸**: í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°ê°€ bfloat16ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì–´ ì •ë°€ë„ ì €í•˜
- **ì›ë³¸**: dtype ë¯¸ì§€ì • (ê¸°ë³¸ float32)
- **ìˆ˜ì •**: `dtype=torch.bfloat16` ì œê±°

### 6. chat_template ì˜¤ë²„ë¼ì´ë“œ ë³µì› (`modeling_memgen.py`)
- **ë¬¸ì œ**: `self.tokenizer.chat_template = CONVERSATION_TEMPLATE` ë¼ì¸ì´ ì£¼ì„ìœ¼ë¡œ ëŒ€ì²´ë¨
- **ì›ì¸**: multi-turn ëŒ€í™” ì‹œ `_is_conversation()`, `_postprocess_assistant_labels()`ê°€ `<|im_start|>` í† í°ì— ì˜ì¡´
- **ìˆ˜ì •**: `self.tokenizer.chat_template = CONVERSATION_TEMPLATE` ë³µì›
- **ì£¼ì˜**: GPT-OSS ë“± ë‹¤ë¥¸ chat template ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì€ `CONVERSATION_TEMPLATE` ìˆ˜ì • í•„ìš”

### 7. mixed_precision ë³µì› (`configs/zero2.yaml`)
- **ë¬¸ì œ**: `mixed_precision: bf16`ìœ¼ë¡œ ë³€ê²½ë¨
- **ì›ë³¸**: `mixed_precision: 'no'` (full precision)
- **ìˆ˜ì •**: `mixed_precision: 'no'`ë¡œ ë³µì›

## Common Commands

### Environment Setup
```bash
conda create -n memgen python=3.10
conda activate memgen
pip install -r requirements.txt
```

### Training

**Train Weaver model (SFT or GRPO):**
```bash
bash scripts/weaver_train.sh
```

**Train Trigger model (GRPO only):**
```bash
bash scripts/trigger_train.sh
```

### Evaluation
```bash
# Update LOAD_MODEL_PATH in eval.sh first
bash scripts/eval.sh
```

### Running with Custom Config
```bash
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/<dataset>.yaml \
    --options <key> <value> ...
```

## Architecture

### Core Components

**MemGenModel** (`memgen/model/modeling_memgen.py`):
- Main model class inheriting from `PreTrainedModel`
- Contains three sub-models: `reasoner` (base LLM), `weaver`, and `trigger`
- Uses LoRA adapters for weaver and trigger to avoid full fine-tuning
- Projection layers (`reasoner_to_weaver`, `weaver_to_reasoner`) map embeddings between components

**MemGenWeaver** (`memgen/model/weaver.py`):
- Generates latent memory tokens via learnable query latents
- Two modes: `augment_prompt()` for prompt-end augmentation, `augment_inference()` for mid-generation augmentation
- Uses `prompt_query_latents` and `inference_query_latents` as trainable parameters

**MemGenTrigger** (`memgen/model/trigger.py`):
- Binary classifier deciding whether to insert memory at each position
- Output layer maps hidden states to 2-class logits (insert/skip)
- When `active=False`, always returns logits favoring insertion

**MemGenRunner** (`memgen/runner.py`):
- Orchestrates training and evaluation
- Two-stage training: weaver first, then trigger
- Supports SFT and GRPO training methods for weaver, GRPO only for trigger

### Data Pipeline

**BaseBuilder** (`data/base_builder.py`):
- Abstract class for dataset construction
- Returns `DatasetDict` with train/valid/test splits
- Provides environment class via `get_env_cls()`

**BaseEnv** (`data/base_env.py`):
- Two environment types: `StaticEnv` (single-turn) and `DynamicEnv` (multi-turn)
- `compute_reward()` method for RL training
- Dynamic envs implement `step()`, `set_env()`, and `feedback()` for interaction loops

**Supported Datasets** (in `data/`):
- `gsm8k`: Math word problems (Static)
- `gpqa`: Graduate-level QA (Static)
- `kodcode`: Code generation (Static)
- `triviaqa`: Retrieval-augmented QA (Dynamic, multi-turn)

### Interaction System

**InteractionManager** (`interactions/base_interaction.py`):
- Manages model generation during training/evaluation
- `SingleTurnInteractionManager`: For static environments
- `MultiTurnInteractionManager`: For dynamic environments with tool use

### Configuration

YAML configs in `configs/latent_memory/` define:
- `model`: Base LLM, weaver/trigger settings, LoRA configs, augmentation parameters
- `dataset`: Dataset name, mode (sft/grpo), validation ratio
- `run`: Training mode, trainer configs (SFT/GRPO hyperparameters), interaction settings

Key augmentation parameters:
- `max_prompt_aug_num`: Number of prompt-end augmentations (1 for reasoning tasks, 6+ for retrieval)
- `max_inference_aug_num`: Number of mid-generation augmentations (5 for reasoning, 0 for retrieval)
- `prompt_latents_len`, `inference_latents_len`: Length of latent sequences

## Key Implementation Details

- Models use `bfloat16` precision and Flash Attention 2
- Training uses Accelerate with DeepSpeed ZeRO-2 (`configs/zero2.yaml`)
- Weaver training fixes trigger params and vice versa via `fix_component()`/`open_component()`
- Multi-turn forward processes conversation turns sequentially, with latents not visible across turns
- Generation loop interleaves trigger decisions with weaver augmentation at delimiter positions

---

## ARC Experiment (2025-01 Update)

### Overview

ARC (Abstract Reasoning Corpus) ì‹¤í—˜ì„ ìœ„í•œ ì½”ë“œ ìƒì„± ê¸°ë°˜ ì ‘ê·¼ë²•. BARC-styleë¡œ ëª¨ë¸ì´ Python ì½”ë“œë¥¼ ìƒì„±í•˜ê³ , training examplesì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ì •í™•ë„ë¡œ rewardë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

### Three Main Execution Modes

#### 1. Pretrain (Weaver SFT/GRPO)
Weaver ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ latent memory ìƒì„± ëŠ¥ë ¥ì„ í›ˆë ¨í•©ë‹ˆë‹¤.

```bash
# SFT Warmup (ê¶Œì¥: GRPO ì „ ì‚¬ì „í•™ìŠµ)
bash scripts/arc_train.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
GPU_IDS=0,1 python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    --num_processes=2 \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode train \
    run.train_weaver true \
    run.train_weaver_method sft  # or grpo
```

**í•™ìŠµ ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.train()` â†’ `_train_weaver()`
2. Weaverì˜ LoRA íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ (trigger ê³ ì •)
3. SFT: supervised learningìœ¼ë¡œ latent ìƒì„± í•™ìŠµ
4. GRPO: code execution accuracyë¥¼ rewardë¡œ ê°•í™”í•™ìŠµ

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/train/arc/<model_name>/`

#### 2. Eval (Standard Evaluation)
í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” base ëª¨ë¸ì˜ ARC ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

```bash
bash scripts/eval.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode evaluate \
    model.load_model_path <checkpoint_path>
```

**í‰ê°€ ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.evaluate()` â†’ `_static_evaluate()`
2. Weaverê°€ latent tokens ìƒì„±
3. Reasonerê°€ latent + promptë¡œ Python ì½”ë“œ ìƒì„±
4. ì½”ë“œ ì‹¤í–‰í•˜ì—¬ training examples ì •í™•ë„ ê³„ì‚°

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/evaluate/arc/<model_name>/evaluate/answer.json`

#### 3. Test-Time Train with LTPO
LTPO (Latent Thought Policy Optimization)ë¥¼ ì‚¬ìš©í•˜ì—¬ inference ì‹œ latentë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.

```bash
bash scripts/eval_ltpo.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python -m accelerate.commands.launch \
    --config_file=configs/zero2.yaml \
    main.py \
    --cfg-path configs/latent_memory/arc.yaml \
    --options \
    run.mode evaluate_ltpo \
    run.ltpo.enabled true \
    run.ltpo.lr 0.03 \
    run.ltpo.max_steps 10
```

**LTPO ìµœì í™” ê³¼ì •:**
1. `main.py` â†’ `MemGenRunner.evaluate_with_ltpo()` â†’ `_static_evaluate_with_ltpo()`
2. Weaverê°€ ì´ˆê¸° latent hidden states ìƒì„±
3. `MemGenLTPOOptimizer.optimize()`:
   - ì´ˆê¸° latentì— noise ì¶”ê°€ (exploration)
   - confidence reward ê³„ì‚° (top-k token probability)
   - gradient ascentë¡œ latent ì—…ë°ì´íŠ¸
   - max_stepsë§Œí¼ ë°˜ë³µ
4. ìµœì í™”ëœ latentë¡œ ì½”ë“œ ìƒì„±

**ì¶œë ¥ ìœ„ì¹˜:** `/data/memgen/evaluate_ltpo/arc/<model_name>/evaluate/answer_ltpo.json`

### LTPO Module (`ltpo/`)

| íŒŒì¼ | ì—­í•  |
|------|------|
| `ltpo.py` | ì›ë³¸ LTPO êµ¬í˜„ (standalone) |
| `memgen_ltpo.py` | MemGen í†µí•© LTPO ìµœì í™”ê¸° |
| `reward.py` | Reward model ì¸í„°í˜ì´ìŠ¤ |

**í•µì‹¬ íŒŒë¼ë¯¸í„° (`configs/latent_memory/arc.yaml`):**
```yaml
run:
  ltpo:
    enabled: true        # LTPO í™œì„±í™”
    lr: 0.03             # ìµœì í™” learning rate
    sigma: 0.1           # exploration noise std
    sigma_decay: 0.99    # noise decay per step
    max_steps: 10        # ìµœëŒ€ ìµœì í™” ìŠ¤í…
    reward_threshold: -1 # early stopping threshold (-1=disabled)
    top_k: 10            # confidence ê³„ì‚°ìš© top-k tokens
    use_auto_grad: true  # PyTorch autograd ì‚¬ìš© (vs REINFORCE)
```

### ARC Environment (`data/arc/env.py`)

| í´ë˜ìŠ¤ | íƒ€ì… | ìš©ë„ |
|--------|------|------|
| `ARCEnv` | Static | Single-turn ì½”ë“œ ìƒì„± + í‰ê°€ |
| `ARCDynamicEnv` | Dynamic | Multi-turn ì½”ë“œ refinement |

**Reward ê³„ì‚°:**
- Binary reward: ALL training examples í†µê³¼ â†’ 1.0, otherwise â†’ 0.0
- `validate_code_on_examples()`: ì½”ë“œ íŒŒì‹± â†’ ì‹¤í–‰ â†’ ì •í™•ë„ ê³„ì‚°

### ì„¤ì • íŒŒì¼

**`configs/latent_memory/arc.yaml`:**
```yaml
model:
  model_name: Qwen/Qwen3-14B
  max_prompt_aug_num: 1      # prompt ë latent ê°œìˆ˜
  max_inference_aug_num: 5   # ìƒì„± ì¤‘ latent ì‚½ì… íšŸìˆ˜
  weaver:
    prompt_latents_len: 8    # prompt latent ê¸¸ì´
    inference_latents_len: 8 # inference latent ê¸¸ì´

dataset:
  name: arc
  data_path: /home/ubuntu/arc-lang-public/data/arc-prize-2024

run:
  mode: train/evaluate/evaluate_ltpo
```

### ì‹¤í–‰ ì›Œí¬í”Œë¡œìš° ìš”ì•½

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Training Flow                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. arc_train.sh â†’ main.py (mode=train)                         â”‚
â”‚  2. MemGenRunner.train() â†’ _train_weaver()                      â”‚
â”‚  3. WeaverGRPOTrainer: prompt â†’ weaver latents â†’ code generationâ”‚
â”‚  4. ARCEnv.compute_reward(): execute code â†’ accuracy â†’ reward   â”‚
â”‚  5. GRPO loss: optimize weaver LoRA parameters                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Evaluation Flow                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. eval.sh â†’ main.py (mode=evaluate)                           â”‚
â”‚  2. MemGenRunner.evaluate() â†’ _static_evaluate()                â”‚
â”‚  3. Weaver generates latents â†’ Reasoner generates code          â”‚
â”‚  4. StaticEvalRecorder: compute_reward() â†’ log results          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LTPO Test-Time Flow                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. eval_ltpo.sh â†’ main.py (mode=evaluate_ltpo)                 â”‚
â”‚  2. MemGenRunner.evaluate_with_ltpo()                           â”‚
â”‚  3. Create MemGenLTPOOptimizer                                  â”‚
â”‚  4. For each sample:                                            â”‚
â”‚     a. Weaver â†’ initial latents                                 â”‚
â”‚     b. LTPO loop: noise â†’ confidence reward â†’ gradient update   â”‚
â”‚     c. Optimized latents â†’ code generation                      â”‚
â”‚  5. Log results to answer_ltpo.json                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” ì½”ë“œ íŒŒì¼

| íŒŒì¼ | ì—­í•  |
|------|------|
| `main.py` | ì§„ì…ì , modeì— ë”°ë¼ train/evaluate/evaluate_ltpo ë¶„ê¸° |
| `memgen/runner.py` | Training/Evaluation orchestration |
| `memgen/model/modeling_memgen.py` | MemGenModel (reasoner + weaver + trigger) |
| `memgen/model/weaver.py` | Latent memory ìƒì„± (augment_prompt/augment_inference) |
| `ltpo/memgen_ltpo.py` | Test-time latent optimization |
| `data/arc/env.py` | ARC environment + reward computation |
| `data/arc/builder.py` | ARC dataset builder |
| `arc/utils.py` | ì½”ë“œ íŒŒì‹±/ì‹¤í–‰/ê²€ì¦ ìœ í‹¸ë¦¬í‹° |

### ë””ë²„ê¹… íŒ

1. **LTPO ìµœì í™” í™•ì¸:** `run.ltpo.verbose: true`ë¡œ ì„¤ì •í•˜ë©´ ê° stepì˜ reward ì¶œë ¥
2. **ì½”ë“œ ì‹¤í–‰ ì—ëŸ¬:** `arc/utils.py`ì˜ `validate_code_on_examples()` ë¡œê·¸ í™•ì¸
3. **ë©”ëª¨ë¦¬ ë¶€ì¡±:** `max_prompt_aug_num`, `max_inference_aug_num` ì¤„ì´ê¸°
4. **Rewardê°€ 0:** training examples JSON íŒŒì‹± í™•ì¸, ì½”ë“œ ë¸”ë¡ í˜•ì‹ í™•ì¸

---

## âš ï¸ ì¤‘ìš” ê°œë… ì •ë¦¬ (2025-01-04)

### Test-Time Optimization vs Test-Time Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    âš ï¸ LTPOëŠ” Test-Time OPTIMIZATIONì´ë‹¤!                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  Test-Time Training (TTT)         â”‚  Test-Time Optimization (LTPO)   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚  â€¢ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ O          â”‚  â€¢ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ X         â”‚
â”‚  â€¢ ì˜êµ¬ì  ë³€ê²½                    â”‚  â€¢ inference ì‹œì—ë§Œ ì„ì‹œ ìµœì í™”    â”‚
â”‚  â€¢ ë³„ë„ êµ¬í˜„ í•„ìš”                 â”‚  â€¢ eval_ltpo.shë¡œ ì‹¤í–‰            â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**LTPOê°€ ìµœì í™”í•˜ëŠ” ê²ƒ:**
- Weaverê°€ ìƒì„±í•œ `latent_hidden_states` (embedding ë²¡í„°)
- ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ **ì¤‘ê°„ í‘œí˜„(latent embeddings)**ë§Œ ìµœì í™”
- ê° ìƒ˜í”Œë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ìµœì í™”, ìƒ˜í”Œ ê°„ ì •ë³´ ê³µìœ  ì—†ìŒ

**LTPO Reward:**
- confidence-based reward (top-k token probability)
- ARC binary reward (code execution accuracy)ì™€ **ë³„ê°œ**

### Binary Reward êµ¬í˜„ (ARC ì „ìš©)

```python
# data/arc/env.py - ARCEnv.compute_reward()
if accuracy == 1.0:   # ëª¨ë“  training examples í†µê³¼
    reward = 1.0
else:                 # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨
    reward = 0.0
```

**ì´ìœ :** ARCì—ì„œ ë¶€ë¶„ ì •ë‹µ(2/3 ë§ìŒ)ì€ ì™„ì „ ì˜¤ë‹µê³¼ ë™ì¼ - ê·œì¹™ì´ ì™„ì „íˆ ë§ê±°ë‚˜ ì™„ì „íˆ í‹€ë¦¬ê±°ë‚˜

### ì„¸ ê°€ì§€ íŒŒì´í”„ë¼ì¸ í•µì‹¬ ì •ë¦¬

| íŒŒì´í”„ë¼ì¸ | ìŠ¤í¬ë¦½íŠ¸ | ëª¨ë¸ ì—…ë°ì´íŠ¸ | ì‚¬ìš© ëª©ì  |
|-----------|---------|-------------|----------|
| **Training** | `weaver_train.sh` | âœ… Yes (LoRA) | Weaver/Trigger í•™ìŠµ |
| **Evaluation** | `eval.sh` | âŒ No | ì„±ëŠ¥ ì¸¡ì • |
| **LTPO Eval** | `eval_ltpo.sh` | âŒ No | Latent ìµœì í™” í›„ í‰ê°€ |

---

## ğŸ”§ ìµœê·¼ ìˆ˜ì • ì‚¬í•­ (2025-01-04)

### Critical Fixes Applied

| # | íŒŒì¼ | ì´ìŠˆ | ìˆ˜ì • |
|---|------|------|------|
| 1 | `memgen/runner.py:110-123` | `_filter_dataset()` evaluate ëª¨ë“œ crash | `interaction_config` fallback ì¶”ê°€ |
| 2 | `data/base_env.py:29` | `preprocess_action(self,...)` | `self` â†’ `cls` (classmethod) |
| 3 | `memgen/trainer/trigger_grpo_trainer.py` | Missing imports | `SamplingParams`, `gather`, `is_conversational` ì¶”ê°€ |
| 4 | `memgen/trainer/trigger_grpo_trainer.py:126-181` | Missing method | `_calculate_rewards()` ë©”ì„œë“œ ì¶”ê°€ |

### ì‚­ì œëœ ì½”ë“œ (ì˜ë„ì )

| íŒŒì¼/í´ë˜ìŠ¤ | ì´ìœ  |
|------------|------|
| `ARCCodeEnv` | `ARCEnv`ì™€ ì¤‘ë³µ (ë™ì¼ ê¸°ëŠ¥) |
| `configs/arc_twostage.yaml` | 2-stage training ë¯¸ì‚¬ìš© |
| `configs/arc_instruction_sft.yaml` | instruction â†’ code ë°©ì‹ ì „í™˜ |
| `interactions/arc_multiturn_interaction.py` | í˜„ì¬ single-turnë§Œ ì‚¬ìš© |

### ìœ ì§€í•´ì•¼ í•  ì½”ë“œ (ì‚­ì œ ê¸ˆì§€!)

| íŒŒì¼ | ì´ìœ  |
|------|------|
| `data/triviaqa/` | ë‹¤ë¥¸ ì‹¤í—˜ìš© dynamic env |
| `interactions/multiturn_interaction.py` | TriviaQA ë“± multi-turn ì§€ì› |
| `ARCDynamicEnv` | í–¥í›„ multi-turn ARC í™•ì¥ìš© |
| `ltpo/` ì „ì²´ | Test-time optimization í•µì‹¬ |

---

## ğŸ“ ARC Single-Turn Code Generation ì ‘ê·¼ë²•

### ì™œ Code Generationì¸ê°€?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BARC-Style Approach                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ê¸°ì¡´ ë°©ì‹ (Instruction)          â”‚  í˜„ì¬ ë°©ì‹ (Code Generation)     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  "ìƒë‹¨ 2ì¤„ì„ í•˜ë‹¨ìœ¼ë¡œ ë³µì‚¬"         â”‚  def main(input_grid):          â”‚
â”‚  â†’ ëª¨í˜¸í•œ ìì—°ì–´ ì§€ì‹œ              â”‚      return input_grid[:2]       â”‚
â”‚  â†’ ì‹¤í–‰ ë¶ˆê°€                      â”‚  â†’ ëª…í™•í•œ ì½”ë“œ                    â”‚
â”‚  â†’ í‰ê°€ ì–´ë ¤ì›€                    â”‚  â†’ ì‹¤í–‰ ê°€ëŠ¥                      â”‚
â”‚                                   â”‚  â†’ ì •í™•ë„ë¡œ í‰ê°€                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ë°ì´í„° íë¦„

```
ARC Task JSON
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCBuilder     â”‚ â†’ training examplesë¥¼ promptë¡œ ë³€í™˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prompt Example:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  Example 1:                                                      â”‚
â”‚  Input (3x3):                                                    â”‚
â”‚  0 0 1                                                           â”‚
â”‚  0 1 0                                                           â”‚
â”‚  1 0 0                                                           â”‚
â”‚                                                                  â”‚
â”‚  Output (3x3):                                                   â”‚
â”‚  1 0 0                                                           â”‚
â”‚  0 1 0                                                           â”‚
â”‚  0 0 1                                                           â”‚
â”‚                                                                  â”‚
â”‚  Write a Python function `main(input_grid)` that implements...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MemGen Model   â”‚ â†’ Weaver latents + Reasoner generation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Generated Code:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  ```python                                                       â”‚
â”‚  def main(input_grid):                                           â”‚
â”‚      import numpy as np                                          â”‚
â”‚      grid = np.array(input_grid)                                 â”‚
â”‚      return np.flip(grid, axis=1).tolist()                       â”‚
â”‚  ```                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ARCEnv Reward  â”‚ â†’ ì½”ë“œ ì‹¤í–‰ â†’ training examples ì •í™•ë„
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Binary Reward: 1.0 (all pass) or 0.0 (any fail)
```

### ì½”ë“œ ì‹¤í–‰ íë¦„ (`arc/utils.py`)

```python
# 1. ì½”ë“œ íŒŒì‹±
code = parse_code_from_text(completion)  # ```python ... ``` ì¶”ì¶œ

# 2. ê° training exampleì—ì„œ ì‹¤í–‰
for example in train_examples:
    result = execute_code_on_input(code, example["input"])
    if result == example["output"]:
        passed += 1

# 3. ì •í™•ë„ ê³„ì‚°
accuracy = passed / total

# 4. Binary reward
reward = 1.0 if accuracy == 1.0 else 0.0
```

---

## ğŸ›¡ï¸ ìˆ˜ì • ì‹œ ì£¼ì˜ì‚¬í•­

### ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ë§ ê²ƒ
1. `ltpo/memgen_ltpo.py` - LTPO í•µì‹¬ ë¡œì§
2. `data/arc/env.py`ì˜ binary reward ë¡œì§
3. `memgen/runner.py`ì˜ `evaluate_with_ltpo()` ë©”ì„œë“œ
4. `main.py`ì˜ mode ë¶„ê¸° ë¡œì§

### ìˆ˜ì • ì „ í™•ì¸í•  ê²ƒ
1. **Import chain**: ìˆœí™˜ ì°¸ì¡° í™•ì¸ (`arc/__init__.py` ì£¼ì˜)
2. **Type signatures**: `@classmethod`ëŠ” `cls` ì‚¬ìš©
3. **Trainer methods**: `_calculate_rewards()` ì¡´ì¬ í™•ì¸
4. **Config keys**: YAML í‚¤ì™€ ì½”ë“œ íŒŒë¼ë¯¸í„°ëª… ì¼ì¹˜ í™•ì¸

### í…ŒìŠ¤íŠ¸ ë°©ë²•
```bash
# ëª¨ë“  import ê²€ì¦
python -c "from memgen.runner import MemGenRunner; from data.arc.env import ARCEnv, ARCDynamicEnv; from ltpo import MemGenLTPOOptimizer; print('OK')"

# LTPO ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
python -c "from memgen.runner import MemGenRunner; assert hasattr(MemGenRunner, 'evaluate_with_ltpo'); print('OK')"
```

---

## ğŸ“š ê´€ë ¨ ë…¼ë¬¸ í•µì‹¬ ìš”ì•½ (2025-01-08)

### LTPO (arXiv:2510.04182)
**ì œëª©**: "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization"

- **ëª©ì **: Test-timeì— latent thought ë²¡í„°ë¥¼ ìµœì í™”í•˜ì—¬ ì¶”ë¡  ì„±ëŠ¥ í–¥ìƒ
- **í•µì‹¬**:
  - **Parameter-free**: ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì—†ìŒ
  - **Confidence-based intrinsic reward**: frozen LLM ì¶œë ¥ ë¶„í¬ì—ì„œ ê³„ì‚°
  - **Noise (sigma)ëŠ” explorationìš©ìœ¼ë¡œ test-timeì—ë§Œ ì¶”ê°€**
  - ì™¸ë¶€ supervisionì´ë‚˜ text generation ì—†ì´ ìµœì í™”

### MemGen (arXiv:2509.24704)
**ì œëª©**: "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents"

- **ëª©ì **: Self-evolving agentë¥¼ ìœ„í•œ generative latent memory í”„ë ˆì„ì›Œí¬
- **í•µì‹¬ ëª¨ë“ˆ**:
  - **Memory Weaver**: í˜„ì¬ ìƒíƒœ â†’ latent token sequence ìƒì„±
  - **Memory Trigger**: memory í˜¸ì¶œ ì‹œì  ê²°ì •
- **ì°¨ë³„ì **: parametric/retrieval memoryì˜ í•œê³„ ê·¹ë³µ, human-like cognitive ability

---

## âš ï¸ Trainingê³¼ LTPOì˜ ëª…í™•í•œ êµ¬ë¶„ (2025-01-08)

### Noise ì ìš© ê·œì¹™

| ëª¨ë“œ | Noise ì ìš© | ì ìš© ìœ„ì¹˜ |
|------|-----------|----------|
| **SFT Training** | âŒ ì—†ìŒ | - |
| **GRPO Training** | âŒ ì—†ìŒ | - |
| **LTPO Eval** | âœ… ì ìš© | `ltpo/memgen_ltpo.py:157-163` |

**í•µì‹¬**: SFT/GRPO í•™ìŠµì—ì„œëŠ” noise ì—†ìŒ. LTPO test-timeì—ì„œë§Œ explorationì„ ìœ„í•´ noise ì¶”ê°€.

### Reward ì‚¬ìš© ê·œì¹™

| ëª¨ë“œ | Reward íƒ€ì… | ìš©ë„ |
|------|-----------|------|
| **SFT Training** | ì—†ìŒ (supervised labels) | Cross-entropy loss |
| **GRPO Training** | Binary (task accuracy) | Policy gradient |
| **LTPO Eval** | Confidence (top-k prob) | Latent optimization |

### ì½”ë“œ íë¦„ í™•ì¸

```
Training (SFT/GRPO):
â”œâ”€â”€ Noise: âŒ ì—†ìŒ
â”œâ”€â”€ Reward: Binary (1.0 or 0.0)
â””â”€â”€ í•™ìŠµ ëŒ€ìƒ: Weaver/Trigger LoRA parameters

Test-Time (LTPO):
â”œâ”€â”€ Noise: âœ… sigmaë¡œ exploration
â”œâ”€â”€ Reward: Confidence-based (top-k token probability)
â””â”€â”€ ìµœì í™” ëŒ€ìƒ: Latent embeddings (ëª¨ë¸ íŒŒë¼ë¯¸í„° X)
```

---

## ğŸ”¬ GSM8K Pipeline ì‹¤í—˜ ê°€ì´ë“œ (`experiments/gsm8k_pipeline/`)

### í™˜ê²½ ì„¤ì •
```bash
# ë°˜ë“œì‹œ memgen conda í™˜ê²½ ì‚¬ìš©
conda activate memgen
```

### ì²´í¬í¬ì¸íŠ¸ ìë™ ê²€ìƒ‰
ê° ìŠ¤í¬ë¦½íŠ¸ëŠ” `common.sh`ë¥¼ í†µí•´ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤:
- `find_latest_weaver_checkpoint()`: ìµœì‹  weaver_lora ê²½ë¡œ ë°˜í™˜
- `find_latest_trigger_checkpoint()`: ìµœì‹  trigger_lora ê²½ë¡œ ë°˜í™˜

### ê°œë³„ ì‹¤í—˜ ì‹¤í–‰ ìˆœì„œ
```bash
# 1. Weaver í•™ìŠµ (SFT)
bash experiments/gsm8k_pipeline/01_weaver_pretrain.sh

# 2. Weaver í‰ê°€ (ìë™ìœ¼ë¡œ ìµœì‹  weaver ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
bash experiments/gsm8k_pipeline/02_eval_weaver.sh

# 3. Trigger í•™ìŠµ (ìë™ìœ¼ë¡œ ìµœì‹  weaver ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
bash experiments/gsm8k_pipeline/03_trigger_pretrain.sh

# 4. Trigger í‰ê°€ (ìë™ìœ¼ë¡œ ìµœì‹  weaver + trigger ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
bash experiments/gsm8k_pipeline/04_eval_trigger.sh

# 5. LTPO í‰ê°€ (ìë™ìœ¼ë¡œ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
bash experiments/gsm8k_pipeline/05_ltpo_eval.sh

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™ ì‹¤í–‰
bash experiments/gsm8k_pipeline/run_all.sh
```

### ìˆ˜ë™ ê²½ë¡œ ì§€ì • (í•„ìš”ì‹œ)
```bash
# ë°©ë²• 1: ì»¤ë§¨ë“œë¼ì¸ ì¸ìë¡œ ì „ë‹¬
bash experiments/gsm8k_pipeline/02_eval_weaver.sh /path/to/weaver_lora
bash experiments/gsm8k_pipeline/03_trigger_pretrain.sh /path/to/weaver_lora
bash experiments/gsm8k_pipeline/04_eval_trigger.sh /path/to/weaver_lora /path/to/trigger_lora
bash experiments/gsm8k_pipeline/05_ltpo_eval.sh /path/to/weaver_lora /path/to/trigger_lora

# ë°©ë²• 2: ìŠ¤í¬ë¦½íŠ¸ ë‚´ ë³€ìˆ˜ ì§ì ‘ ìˆ˜ì •
LOAD_WEAVER_PATH="/path/to/weaver_lora"
LOAD_TRIGGER_PATH="/path/to/trigger_lora"
```

### ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ìœ„ì¹˜
- **í•™ìŠµ**: `~/data/memgen/train/<dataset>/<model_name>/pn=*_pl=*_in=*_il=*_<timestamp>/`
- **í‰ê°€**: `~/data/memgen/evaluate/<dataset>/<model_name>/.../evaluate/answer.json`
- **LTPO**: `~/data/memgen/evaluate_ltpo/<dataset>/<model_name>/.../evaluate/answer_ltpo.json`

### í•µì‹¬ ì½”ë“œ ê²½ë¡œ ì°¸ì¡°
| ê¸°ëŠ¥ | íŒŒì¼ ìœ„ì¹˜ |
|------|----------|
| LTPO optimizer | `ltpo/memgen_ltpo.py:110-213` |
| Noise ì ìš© | `ltpo/memgen_ltpo.py:157-163` |
| GRPO reward | `memgen/trainer/weaver_grpo_trainer.py:186-241` |
| Binary reward | `data/arc/env.py:107-116` |
